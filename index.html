<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>机器学习从入门到放弃</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="机器学习从入门到放弃">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="机器学习从入门到放弃">
<meta property="og:locale" content="zh-cn">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习从入门到放弃">
  
    <link rel="alternate" href="/atom.xml" title="机器学习从入门到放弃" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">机器学习从入门到放弃</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">从CUDA到cuDNN到进入疯人院</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Suche"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-0x11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/03/18/0x11/" class="article-date">
  <time datetime="2018-03-17T16:00:00.000Z" itemprop="datePublished">2018-03-18</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/03/18/0x11/">0x11 OPENCL编程指南前言</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>During the past few years, heterogeneous computerscomposed of CPUs and GPUs have revolutionized computing. By matching different parts of a workload to the most suitable processor, tremendous performance gains have been achieved.</p>
<hr>
<p>在过去几年里，各种各样的由CPU和GPU组成的计算机迎来了计算革命，通过把不同的工作分配给最适合的处理器，可以获得极高的性能。</p>
<p>heterogeneous: 各种各样的、不同的</p>
<p>tremendous: 巨大的，极大的，极强的</p>
<hr>
<p>Much of this revolution has been driven by the emergence of many-core processors such as GPUs. Forexample, it is now possible to buy a graphics card that can execute more than a trillion floating point operation per second(teraflops). These GPUs were designed to render beautiful images, but for the right workloads, they can also be used as high-performance computing engines for applications from scientific computing to augmented reality.</p>
<hr>
<p>这种革命大多是由多核处理器带来的，例如GPU。举个栗子，现在买一张显卡就能执行每秒超过一万亿次浮点指令。这些GPU本来设计用来渲染漂亮的图片，但是通过适当的调度，也可以用来执行高性能计算，上至科学计算，下至增强现实。</p>
<hr>
<p>A natural question is why these many-core processors are so fast compared to traditional single core CPUs. The fundamental driving force is innovative parallel hardware. Parallel computing is more efficient than sequential computing because chips are fundamentally parallel. Modern chips contain billions of transistors. Many-core processors organize these transistors into many parallel processors consisting of hundreds of floating point units. Another important reason for their speed advantage is new parallel software. Utilizing all these computing resources requires that we develop parallel programs. The efficiency gains due to software and hardware, allow us to get more FLOPs per Watt or per dollay than a single-core CPU.</p>
<hr>
<p>自然而然会想到，为什么这些多核处理器和传统单核处理器相比，会快上这么多呢？最基本的原因就是这是一种创新性的并行硬件。在并行处理器上并行计算比串行计算要高效的多。现代的芯片是由数十亿的晶体管组成的。而多喝处理器把这些晶体管组织成大量并行处理器，包含上百个浮点单元。另一个重要的原因就是全新的并行软件放大了这方面的优势。为了更好地利用这些计算资源，我们开发了并行编程。从硬件软件两方面得到的效率，比起单核处理器的能效比、或者是能费比要高得多。</p>
<p>fundamental: 基础的，根本的。</p>
<p>innovative: 革新的,创新的。</p>
<p>transistors: 晶体(三级)管,晶体收音机。</p>
<p>Utilizing：利用，运用。</p>
<ul>
<li>能效比：浮点数每秒每瓦特</li>
<li>能费比：浮点数每秒每美元</li>
</ul>
<hr>
<p>Computing systems are a symbiotic combination of hardware and software. Hardware is not useful without a good programming model. The success of CPUs has been tied to the success of their programming models, as exemplified by C language and its successor. C nicely abstracts a sequential computer. To fully exploit heterogeneous computers, we need new programming models that nicely abstract a modern <em>parallel</em> computer. And we can look to techniques established in graphics as a guide to the new programming models we need for heterogeneous computing.</p>
<hr>
<p>计算系统是由硬件和软件共同组合而成的。 硬件要是没有一个好的编程模型就没啥用。CPU的成功和它的编程模型的成功密切相关，就像C语言和它的继任者一样典型。C语言对顺序计算进行了很好的抽象化。为了完美地开发出不同计算机的能力，就必须为现代的并行计算机创建一个良好的抽象模型。我们可以把显卡里面使用的技术当作参考，来建立我们所需要的异构计算的模型。</p>
<p>symbiotic: 共生的，共性的。</p>
<p>exemplified： 是…的典范，典型</p>
<p>exploit： 开发，利用，开采</p>
<hr>
<p>I have been interested in programming models for graphics for many years. It started in 1988 when I was a software engineer at PIXAR, where I developed the RenderMan shading language. A decade later graphics system became fast enough that we could consider developing shading languages for GPUs. With Kekoa Proudfoot and Bill Mark, we developed a real-time shading language, RTSL. RTSL ran on graphics hardware by compiling shading language programs into pixel shader programs, the assembly language for graphics hardware of the day. Bill Mark subsequently went to work at NVIDIA, where he developed Cg. More recently, I have been working with Tim Foley at Intel, who has developed a new shading language called Spark. Spark takes shading languages to the next level by abstracting complex graphics pipelines with new capabilities such as tesselation.</p>
<hr>
<p>我已经对显卡的计算模型感兴趣好多年了。最开始的时候是1988年，那时我还在皮克斯当一个软件工程师，在那我开发了一个叫RenderMan的着色器语言。十年之后，图形系统已经快到我们觉得可以为GPU开发一个着色器语言。当时伙同Kekoa Proudfoot、Bill Mark，我们开发了一个实时着色器语言，RTSL。RTSL利用着色器程序编译成为pixel shader程序，以便在显卡上工作。Bill Mark随后就前往英伟达就职，在那他开发了Cg工具包。最近，我和Tim Foley 在英特尔工作，他在开发过一个新的着色器语言叫做Spark。Spark通过抽象化复杂的图形管线，把着色器语言提升了一个层次，还添加了诸如曲面细分之类的新功能。</p>
<p>decade：十年，年代</p>
<p>More recently： 近来，最近。</p>
<p>capabilities：能力，才能.</p>
<p>tesselation： 曲面细分技术。</p>
<hr>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/03/18/0x11/" data-id="cjf14qots0007xstm2ii70tz2" class="article-share-link">Teilen</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-0x10" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/03/09/0x10/" class="article-date">
  <time datetime="2018-03-09T05:41:29.982Z" itemprop="datePublished">2018-03-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/03/09/0x10/">数学笔记</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="非齐次一阶线性微分方程解法"><a href="#非齐次一阶线性微分方程解法" class="headerlink" title="非齐次一阶线性微分方程解法"></a>非齐次一阶线性微分方程解法</h2><ol>
<li>变换为对应的齐次线性方程</li>
<li>分离变量，两端积分</li>
<li>常数变易法，变C为U</li>
<li>新式子两端求导，将得到的y和y’代会原式</li>
<li>销项之后得到u’，两端积分得到u</li>
<li>将u代回步骤3得到的式子</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/03/09/0x10/" data-id="cjf14qotp0005xstma7rt0dmc" class="article-share-link">Teilen</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-0x09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/01/01/0x09/" class="article-date">
  <time datetime="2017-12-31T16:00:00.000Z" itemprop="datePublished">2018-01-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/01/0x09/">0x09 CUDA内存管理</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>CUDA的内存管理与C语言的类似，需要程序员显式地进行内存的分配、释放、移动，在此之前，已经使用过了四个CUDA内存相关的接口：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMalloc</span><span class="params">(T **devPtr, <span class="keyword">size_t</span> size)</span></span>;</span><br><span class="line"><span class="function">cudaError_t <span class="title">cudaFree</span><span class="params">(<span class="keyword">void</span> *devPtr)</span></span>;</span><br><span class="line"><span class="function">cudaError_t <span class="title">cudaMemset</span><span class="params">(<span class="keyword">void</span> *devPtr, <span class="keyword">int</span> value, <span class="keyword">size_t</span> count)</span></span>;</span><br><span class="line"><span class="function">cudaError_t <span class="title">cudaMemcpy</span><span class="params">(<span class="keyword">void</span> *dst, <span class="keyword">const</span> <span class="keyword">void</span> *src, <span class="keyword">size_t</span> count, <span class="keyword">enum</span> cudaMemcpyKind kind)</span></span>;</span><br></pre></td></tr></table></figure></p>
<p>以及cudaMemcpy函数的四个拷贝方向：</p>
<ul>
<li>cudaMemcpyHostToHost</li>
<li>cudaMemcpyHostToDevice</li>
<li>cudaMemcpyDeviceToHost</li>
<li>cudaMemcpyDeviceToDevice</li>
<li>cudaMemcpyDefault</li>
</ul>
<p><del>四大拷贝方向当然有五个</del><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"cuda_runtime.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdlib&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">unsigned</span> <span class="keyword">int</span> nsize = <span class="number">1</span> &lt;&lt; <span class="number">22</span>;</span><br><span class="line">	<span class="keyword">unsigned</span> <span class="keyword">int</span> nbytes = nsize*<span class="keyword">sizeof</span>(<span class="keyword">float</span>);</span><br><span class="line">	<span class="keyword">float</span> *h_a = (<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nbytes);</span><br><span class="line">	<span class="keyword">float</span> *d_a;</span><br><span class="line">	cudaMalloc(&amp;d_a, nbytes);</span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nsize; i++)</span><br><span class="line">		h_a[i] = <span class="number">3.14159f</span>;</span><br><span class="line">	cudaMemcpy(d_a, h_a, nbytes, cudaMemcpyHostToDevice);</span><br><span class="line">	cudaMemcpy(h_a, d_a, nbytes, cudaMemcpyDeviceToHost);</span><br><span class="line">	cudaFree(d_a);</span><br><span class="line">	<span class="built_in">free</span>(h_a);</span><br><span class="line">	cudaDeviceReset();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这是一个简单的数据拷贝的DEMO，利用nvprof可以查看它的运行情况：</p>
<table><br>    <tbody><br>        <tr><br>            <th>Type</th><br>            <th>Time(%)</th><br>            <th>Time</th><br>            <th>Calls</th><br>            <th>Avg</th><br>            <th>Min</th><br>            <th>Max</th><br>            <th>Name</th><br>        </tr><br>        <tr><br>            <td>GPU activities:</td><br>            <td>50.83%</td><br>            <td>3.5437ms</td><br>            <td>1</td><br>            <td>3.5437ms</td><br>            <td>3.5437ms</td><br>            <td>3.5437ms</td><br>            <td>[CUDA memcpy HtoD]</td><br>        </tr><br>        <tr><br>            <td></td><br>            <td>49.17%</td><br>            <td>3.4280ms</td><br>            <td>1</td><br>            <td>3.4280ms</td><br>            <td>3.4280ms</td><br>            <td>3.4280ms</td><br>            <td>[CUDA memcpy DtoH]</td><br>        </tr><br>    </tbody><br></table>

<p>可以简单的计算一下，4M个浮点数占16MB空间，大约传输速率为4.515Gb/s，然而GPU和GDDR5显存之间理论带宽高达上百Gb/s，差距是相当巨大的。</p>
<h2 id="固定内存"><a href="#固定内存" class="headerlink" title="固定内存"></a>固定内存</h2><p>主机的内存默认是pageable，也就是说内存被操作系统分页处理，这个操作会使得操作系统将主机上的虚拟内存分别移动到不同的物理位置。因此，GPU不可能在可分页主机内存上面安全地访问数据，当需要传输数据时，CUDA驱动程序首先分配一段锁定的分页或者固定的主机内存，然后将需要传递的主机原数据放在固定内存中，最后，再传输到设备内存。</p>
<p>而CUDA运行时允许程序员通过下面的指令手动分配固定主机内存：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMallocHost</span><span class="params">(<span class="keyword">void</span> **devPtr, <span class="keyword">size_t</span> count)</span></span>;</span><br></pre></td></tr></table></figure></p>
<p>分配的这些内存是页面锁定的，也就是不会被操作系统所移动，因此可以直接被设备访问，但是分配过多会导致主机的性能下降。</p>
<p>这些内存通过下面的函数释放：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaFreeHost</span><span class="params">(<span class="keyword">void</span> *ptr)</span></span></span><br></pre></td></tr></table></figure></p>
<p>接下来我们用固定内存替换掉可分页内存试试,仅仅需要把普通的malloc和free替换为cudaMallocHost和cudaFreeHost即可；</p>
<table><br>    <tbody><br>        <tr><br>            <th>Type</th><br>            <th>Time(%)</th><br>            <th>Time</th><br>            <th>Calls</th><br>            <th>Avg</th><br>            <th>Min</th><br>            <th>Max</th><br>            <th>Name</th><br>        </tr><br>        <tr><br>            <td>GPU activities:</td><br>            <td>50.40%</td><br>            <td>2.5224ms</td><br>            <td>1</td><br>            <td>2.5224ms</td><br>            <td>2.5224ms</td><br>            <td>2.5224ms</td><br>            <td>[CUDA memcpy HtoD]</td><br>        </tr><br>        <tr><br>            <td></td><br>            <td>49.60%</td><br>            <td>2.4822ms</td><br>            <td>1</td><br>            <td>2.4822ms</td><br>            <td>2.4822ms</td><br>            <td>2.4822ms</td><br>            <td>[CUDA memcpy DtoH]</td><br>        </tr><br>    </tbody><br></table>

<p>从3.5ms提升到2.5ms了，性能提升了40%，带宽提升到了6.343Gb/S</p>
<h2 id="零拷贝内存"><a href="#零拷贝内存" class="headerlink" title="零拷贝内存"></a>零拷贝内存</h2><p>一般来说，主机和设备之间不能相互访问对方的变量，但是有一个例外：零拷贝内存。这是一个主机和设备都可以访问的内存。在CUDA核函数之中使用零拷贝内存有着以下优势：</p>
<ul>
<li>设备内存不足的时候可以利用主机的内存</li>
<li>避免主机和设别之间数据的显式传输</li>
<li>提高PCIe传输率</li>
</ul>
<p>可以通过下面的函数开辟一片零拷贝内存：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaHostAlloc</span><span class="params">(<span class="keyword">void</span> **pHost, <span class="keyword">size_t</span> count, <span class="keyword">unsigned</span> <span class="keyword">int</span> flags)</span></span>;</span><br></pre></td></tr></table></figure>
<p>这个函数分配了count字节的主机内存，这片内存是页面锁定的，且可以被设备访问。这个函数分配的内存必须使用cudaFreeHost函数释放。flags参数可选项如下：</p>
<ul>
<li>cudaHostAllocDefalt</li>
</ul>
<p>使得此函数的行为和cudaMallocHost函数一致</p>
<ul>
<li>cudaHostAllocPortable</li>
</ul>
<p>使得分配的内存可以被所有CUDA上下文所使用，而不只是执行分配的那个</p>
<ul>
<li>cudaHostAllocWriteCombined</li>
</ul>
<p>可以免除cpu对内存的监视，减少将内存上的数据缓存到L1、L2中，在主机和设备之间传输时节省了时间，会导致主机读取这片内存效率低下。适用于主机只写入数据的场合</p>
<ul>
<li>cudaHostAllocMapped</li>
</ul>
<p>分配写联合的存储空间，主要用于主机到设备的传输或者通过映射页锁定空间CPU写而设备读的情况。但CPU读的效率不高。<br>在开辟零拷贝内存空间之后，可以通过下面的函数获取映射到此内存的设备指针：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaHostGetDevicePointer</span><span class="params">(<span class="keyword">void</span> **pDevice, <span class="keyword">void</span> *pHost, <span class="keyword">unsigned</span> <span class="keyword">int</span> flags)</span></span>;</span><br></pre></td></tr></table></figure>
<p>这个函数获取到的pDevice就是设备指针，可以像普通设备指针一样在设备上引用。但是要注意的是，PCIe总线速度远低于GDDR5内存速度，因此使用零拷贝内存会导致核函数性能降低。</p>
<p>下面是一个利用零拷贝内存执行两个一维数组相加的DEMO:<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdlib&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstring&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">initialData</span><span class="params">(<span class="keyword">float</span> *ip, <span class="keyword">int</span> size)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> i;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; size; i++)</span><br><span class="line">	&#123;</span><br><span class="line">		ip[i] = (<span class="keyword">float</span>)(rand() &amp; <span class="number">0xFF</span>) / <span class="number">10.0f</span>;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">sumArraysOnHost</span><span class="params">(<span class="keyword">float</span> *A, <span class="keyword">float</span> *B, <span class="keyword">float</span> *C, <span class="keyword">const</span> <span class="keyword">int</span> N)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> idx = <span class="number">0</span>; idx &lt; N; idx++)</span><br><span class="line">	&#123;</span><br><span class="line">		C[idx] = A[idx] + B[idx];</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">sumArrays</span><span class="params">(<span class="keyword">float</span> *A, <span class="keyword">float</span> *B, <span class="keyword">float</span> *C, <span class="keyword">const</span> <span class="keyword">int</span> N)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> i = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> (i &lt; N) C[i] = A[i] + B[i];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">sumArraysZeroCopy</span><span class="params">(<span class="keyword">float</span> *A, <span class="keyword">float</span> *B, <span class="keyword">float</span> *C, <span class="keyword">const</span> <span class="keyword">int</span> N)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> i = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> (i &lt; N) C[i] = A[i] + B[i];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="comment">// set up data size of vectors</span></span><br><span class="line">	<span class="keyword">int</span> ipower = <span class="number">10</span>;</span><br><span class="line">	<span class="keyword">if</span> (argc &gt; <span class="number">1</span>) ipower = atoi(argv[<span class="number">1</span>]);</span><br><span class="line">	<span class="keyword">int</span> nElem = <span class="number">1</span> &lt;&lt; ipower;</span><br><span class="line">	<span class="keyword">size_t</span> nBytes = nElem * <span class="keyword">sizeof</span>(<span class="keyword">float</span>);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"Vector size %d power %d  nbytes  %3.0f KB\n"</span>, nElem, ipower,(<span class="keyword">float</span>)nBytes / (<span class="number">1024.0f</span>));</span><br><span class="line"></span><br><span class="line">	<span class="comment">// part 1: using device memory</span></span><br><span class="line">	<span class="comment">// malloc host memory</span></span><br><span class="line">	<span class="keyword">float</span> *h_A, *h_B, *hostRef, *gpuRef;</span><br><span class="line">	h_A = (<span class="keyword">float</span> *)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">	h_B = (<span class="keyword">float</span> *)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">	hostRef = (<span class="keyword">float</span> *)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">	gpuRef = (<span class="keyword">float</span> *)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line"></span><br><span class="line">	<span class="comment">// initialize data at host side</span></span><br><span class="line">	initialData(h_A, nElem);</span><br><span class="line">	initialData(h_B, nElem);</span><br><span class="line">	<span class="built_in">memset</span>(hostRef, <span class="number">0</span>, nBytes);</span><br><span class="line">	<span class="built_in">memset</span>(gpuRef, <span class="number">0</span>, nBytes);</span><br><span class="line"></span><br><span class="line">	<span class="comment">// add vector at host side for result checks</span></span><br><span class="line">	sumArraysOnHost(h_A, h_B, hostRef, nElem);</span><br><span class="line"></span><br><span class="line">	<span class="comment">// malloc device global memory</span></span><br><span class="line">	<span class="keyword">float</span> *d_A, *d_B, *d_C;</span><br><span class="line">	cudaMalloc((<span class="keyword">float</span>**)&amp;d_A, nBytes);</span><br><span class="line">	cudaMalloc((<span class="keyword">float</span>**)&amp;d_B, nBytes);</span><br><span class="line">	cudaMalloc((<span class="keyword">float</span>**)&amp;d_C, nBytes);</span><br><span class="line"></span><br><span class="line">	<span class="comment">// transfer data from host to device</span></span><br><span class="line">	cudaMemcpy(d_A, h_A, nBytes, cudaMemcpyHostToDevice);</span><br><span class="line">	cudaMemcpy(d_B, h_B, nBytes, cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line">	<span class="comment">// set up execution configuration</span></span><br><span class="line">	<span class="keyword">int</span> iLen = <span class="number">512</span>;</span><br><span class="line">	<span class="function">dim3 <span class="title">block</span><span class="params">(iLen)</span></span>;</span><br><span class="line">	dim3 grid((nElem + block.x - 1) / block.x);</span><br><span class="line"></span><br><span class="line">	sumArrays &lt;&lt; &lt;grid, block &gt;&gt; &gt;(d_A, d_B, d_C, nElem);</span><br><span class="line"></span><br><span class="line">	<span class="comment">// copy kernel result back to host side</span></span><br><span class="line">	cudaMemcpy(gpuRef, d_C, nBytes, cudaMemcpyDeviceToHost);</span><br><span class="line">	</span><br><span class="line">	<span class="comment">// free device global memory</span></span><br><span class="line">	cudaFree(d_A);</span><br><span class="line">	cudaFree(d_B);</span><br><span class="line"></span><br><span class="line">	<span class="comment">// free host memory</span></span><br><span class="line">	<span class="built_in">free</span>(h_A);</span><br><span class="line">	<span class="built_in">free</span>(h_B);</span><br><span class="line"></span><br><span class="line">	<span class="comment">// part 2: using zerocopy memory for array A and B</span></span><br><span class="line">	<span class="comment">// allocate zerocpy memory</span></span><br><span class="line">	cudaHostAlloc((<span class="keyword">void</span> **)&amp;h_A, nBytes, cudaHostAllocMapped);</span><br><span class="line">	cudaHostAlloc((<span class="keyword">void</span> **)&amp;h_B, nBytes, cudaHostAllocMapped);</span><br><span class="line"></span><br><span class="line">	<span class="comment">// initialize data at host side</span></span><br><span class="line">	initialData(h_A, nElem);</span><br><span class="line">	initialData(h_B, nElem);</span><br><span class="line">	<span class="built_in">memset</span>(hostRef, <span class="number">0</span>, nBytes);</span><br><span class="line">	<span class="built_in">memset</span>(gpuRef, <span class="number">0</span>, nBytes);</span><br><span class="line"></span><br><span class="line">	<span class="comment">// pass the pointer to device</span></span><br><span class="line">	cudaHostGetDevicePointer((<span class="keyword">void</span> **)&amp;d_A, (<span class="keyword">void</span> *)h_A, <span class="number">0</span>);</span><br><span class="line">	cudaHostGetDevicePointer((<span class="keyword">void</span> **)&amp;d_B, (<span class="keyword">void</span> *)h_B, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">	<span class="comment">// add at host side for result checks</span></span><br><span class="line">	sumArraysOnHost(h_A, h_B, hostRef, nElem);</span><br><span class="line"></span><br><span class="line">	<span class="comment">// execute kernel with zero copy memory</span></span><br><span class="line">	sumArraysZeroCopy &lt;&lt; &lt;grid, block &gt;&gt; &gt;(d_A, d_B, d_C, nElem);</span><br><span class="line"></span><br><span class="line">	<span class="comment">// copy kernel result back to host side</span></span><br><span class="line">	cudaMemcpy(gpuRef, d_C, nBytes, cudaMemcpyDeviceToHost);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	<span class="comment">// free  memory</span></span><br><span class="line">	cudaFree(d_C);</span><br><span class="line">	cudaFreeHost(h_A);</span><br><span class="line">	cudaFreeHost(h_B);</span><br><span class="line">	<span class="built_in">free</span>(hostRef);</span><br><span class="line">	<span class="built_in">free</span>(gpuRef);</span><br><span class="line"></span><br><span class="line">	<span class="comment">// reset device</span></span><br><span class="line">	cudaDeviceReset();</span><br><span class="line">	<span class="keyword">return</span> EXIT_SUCCESS;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>利用nvprof工具可以看到零拷贝内存和传统方法的性能对比：</p>
<table><br>    <tbody><br>        <tr><br>            <th>Type</th><br>            <th>Time(%)</th><br>            <th>Time</th><br>            <th>Calls</th><br>            <th>Avg</th><br>            <th>Min</th><br>            <th>Max</th><br>            <th>Name</th><br>        </tr><br>        <tr><br>            <td>GPU activities:</td><br>            <td>21.67%</td><br>            <td>29.010ms</td><br>            <td>1</td><br>            <td>29.010ms</td><br>            <td>29.010ms</td><br>            <td>29.010ms</td><br>            <td>sumArraysZeroCopy(float<br>                <em>, float</em>, float<br>                <em>, int)</em><br>            </td><br>        </tr><br>        <tr><br>            <td></td><br>            <td>18.14%</td><br>            <td>17.601ms</td><br>            <td>1</td><br>            <td>17.601ms</td><br>            <td>17.601ms</td><br>            <td>17.601ms</td><br>            <td>sumArrays(float, float<br>                <em>, float</em>, int)</td><br>        </tr><br>    </tbody><br></table>

<p>可以看到使用零拷贝内存的函数，和传统方法相比效率稍微差了一些。因此，如果想要共享主机和设备之间的少量数据，零拷贝内存是一个不错的方法。因为它简化了代码量且有着不算太差的性能。但是太大的数据集而言，可能并不是一个很好的选择。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/01/01/0x09/" data-id="cjf14qotw000axstm6rp0gxf6" class="article-share-link">Teilen</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-0x07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/12/30/0x07/" class="article-date">
  <time datetime="2017-12-29T16:00:00.000Z" itemprop="datePublished">2017-12-30</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/12/30/0x07/">0x07 延迟隐藏和GPU占用率</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>与CPU上的C语言编程相比，在CUDA编程之中延迟隐藏显得尤为重要，这是因为CPU和GPU从本初的设计概念开始就有着极大的不同。GPU注重并行计算和吞吐量的最大化，将绝大多数的芯片面积用于计算单元。而CPU的设计理念在于一两个线程的延迟最小化，大量的芯片面积用于构建缓存，而且还拥有着一套非常复杂且有效的分支预测和流水线结构。不过好在，GPU上的延迟，可以被大量线程束的并行化所隐藏。</p>
<p>考虑到指令延迟，可以分为两个大类：</p>
<ul>
<li>算数指令</li>
<li>内存指令</li>
<li>算数指令延迟指的是一个算数操作从发出指令开始到产生输出之间的时间间隔，大约在10~20个周期。而内存指令延迟值得是送出的内存读写操作，和数据到达目的地之间的时间间隔，大约花费400~800个周期。<br>那么要隐藏延迟，需要多少个活跃的线程束呢？利特尔法则(Little’s Law)可以给出一个合理的近似值：<br>所需线程束数量=延迟*吞吐量</li>
</ul>
<p><strong>举一个形象的例子，你是一个小厂的厂长，总公司要求你每天交上去20匹织好的布(吞吐量)，每匹布需要一个工人工作5天(延迟)，那么，你需要同时招聘多少个工人才能满足总公司的需求呢？(所需的活跃线程束数量)，显而易见的需要100个工人。</strong><br>// TODO:<br>对一维数组求和的并行化</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/12/30/0x07/" data-id="cjf14qotu0008xstml84tq9ys" class="article-share-link">Teilen</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-0x08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/12/30/0x08/" class="article-date">
  <time datetime="2017-12-29T16:00:00.000Z" itemprop="datePublished">2017-12-30</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/12/30/0x08/">0x08 CUDA内存模型</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><del>这节我本来都不想写了，先写个梗概把</del></p>
<ul>
<li>计算机的两个局部性<ul>
<li>时间局部性</li>
<li>空间局部性</li>
</ul>
</li>
<li>计算机储存的层级结构<ul>
<li>寄存器</li>
<li>缓存</li>
<li>主存</li>
<li>外存</li>
</ul>
</li>
</ul>
<hr>
<p>OK，复习完毕，如果就上面几个关键词不能写个八百字说明文的话，那就回去翻翻书吧。</p>
<hr>
<p>CUDA的内存层级结构中，提出来多种可编程内存类型：</p>
<ul>
<li>寄存器</li>
<li>共享内存</li>
<li>本地内存</li>
<li>常量内存</li>
<li>纹理内存</li>
<li>全局内存<h2 id="寄存器"><a href="#寄存器" class="headerlink" title="寄存器"></a>寄存器</h2></li>
<li>寄存器是GPU上速度最快的内存空间</li>
<li>在一个核函数中声明的，没有任何其他修饰的变量，大多会被放在寄存器里面，定长数组也会放在寄存器里。</li>
<li>但是在Fermi GPU中每个线程仅允许63个寄存器，kepler GPU则提升到255个。如果超出了这个范围，则会被放在本地内存</li>
<li>寄存器变量对于每个变量都是私有的，且和该线程生命周期相同。(也就是说和普通C函数里面的普通变量相似)<h2 id="本地内存"><a href="#本地内存" class="headerlink" title="本地内存"></a>本地内存</h2>寄存器的接盘侠，主要接的盘有：</li>
<li>变长数组</li>
<li>可能占用过多寄存器空间的结构体或定长数组。</li>
<li>从寄存器溢出的变量</li>
<li>访问权限和生命周期都禹寄存器变量相同</li>
<li>物理上的所在位置在片外，和全局内存一样，所以尽量避免使用<h2 id="共享内存"><a href="#共享内存" class="headerlink" title="共享内存"></a>共享内存</h2></li>
<li>利用shared修饰符，在核函数内声明，可以把变量放在共享内存中</li>
<li>共享内存是片上内存，所以与本地内存和全局内存相比，有着更高的带宽和更低的延迟。</li>
<li>每一个SM的共享内存有限，如果过度使用，就会限制活跃线程束的数量</li>
<li>共享内存的访问权限是这个线程块，生命周期也和其线程块相同，当一个线程块里面所有线程束执行完毕之后，所占用的内存会被自动释放并分配</li>
<li>如果要利用共享内存进行线程间通信，请注意必须调用 void __syncthreads(); 避免脏读<h2 id="常量内存"><a href="#常量内存" class="headerlink" title="常量内存"></a>常量内存</h2></li>
<li>驻留在设备内存中，并且在SM专用的常量缓存中缓存</li>
<li>使用修饰符 <strong>constant</strong>修饰</li>
<li>必须在全局空间内和所有核函数之外进行说明。</li>
<li>常量内存是静态说明的，并对所有核函数可见，但是只读</li>
<li><p>常量内存必须在主机端利用以下函数进行初始化：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMemcpyToSymbol</span><span class="params">(<span class="keyword">const</span> <span class="keyword">void</span>* symbol, <span class="keyword">const</span> <span class="keyword">void</span>* src, <span class="keyword">size_t</span> count)</span></span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>每次从常量内存中读取数据，都会广播给整个线程束</p>
</li>
</ul>
<h2 id="纹理内存"><a href="#纹理内存" class="headerlink" title="纹理内存"></a>纹理内存</h2><p><del>没看懂……看懂了补上</del></p>
<h2 id="全局内存"><a href="#全局内存" class="headerlink" title="全局内存"></a>全局内存</h2><ul>
<li>是整个GPU里面最大，延迟最高，且最常使用到的内存。</li>
<li>使用<strong>device</strong>修饰符静态地说明一个变量</li>
<li>使用cudaMalloc()动态声明一个变量，用cudaFree()释放它</li>
<li>可以被所有SM所访问，且生命周期贯穿整个应用程序</li>
</ul>
<hr>
<h3 id="CUDA变量及其类型修饰符"><a href="#CUDA变量及其类型修饰符" class="headerlink" title="CUDA变量及其类型修饰符"></a>CUDA变量及其类型修饰符</h3><table><br>    <tbody><br>        <tr><br>            <th>修饰符</th><br>            <th>是否为数组</th><br>            <th>储存器</th><br>            <th>作用域</th><br>            <th>生命周期</th><br>        </tr><br>        <tr><br>            <td></td><br>            <td>N</td><br>            <td>寄存器</td><br>            <td>线程</td><br>            <td>线程</td><br>        </tr><br>        <tr><br>            <td></td><br>            <td>Y</td><br>            <td>本地</td><br>            <td>线程</td><br>            <td>线程</td><br>        </tr><br>        <tr><br>            <td><br>                <strong>shared</strong><br>            </td><br>            <td>Y/N</td><br>            <td>共享</td><br>            <td>块</td><br>            <td>块</td><br>        </tr><br>        <tr><br>            <td><br>                <strong>device</strong><br>            </td><br>            <td>Y/N</td><br>            <td>全局</td><br>            <td>全局</td><br>            <td>应用程序</td><br>        </tr><br>        <tr><br>            <td><br>                <strong>constant</strong><br>            </td><br>            <td>Y/N</td><br>            <td>常量</td><br>            <td>全局</td><br>            <td>应用程序</td><br>        </tr><br>    </tbody><br></table>

<h3 id="设备储存器的主要特征"><a href="#设备储存器的主要特征" class="headerlink" title="设备储存器的主要特征"></a>设备储存器的主要特征</h3><p><table><br>    <tbody><br>        <tr><br>            <th>储存器</th><br>            <th>片上/片外</th><br>            <th>缓存</th><br>            <th>存取权限</th><br>            <th>范围</th><br>            <th>生命周期</th><br>        </tr><br>        <tr><br>            <td>寄存器</td><br>            <td>片上</td><br>            <td>N/A</td><br>            <td>R/W</td><br>            <td>一个线程</td><br>            <td>线程</td><br>        </tr><br>        <tr><br>            <td>本地</td><br>            <td>片外</td><br>            <td>计算能力2.0以上有</td><br>            <td>R/W</td><br>            <td>一个线程</td><br>            <td>线程</td><br>        </tr><br>        <tr><br>            <td>共享</td><br>            <td>片上</td><br>            <td>N/A</td><br>            <td>R/W</td><br>            <td>块内所有线程</td><br>            <td>块</td><br>        </tr><br>        <tr><br>            <td>全局</td><br>            <td>片外</td><br>            <td>计算能力2.0以上有</td><br>            <td>R/W</td><br>            <td>所有线程+主机</td><br>            <td>主机配置</td><br>        </tr><br>        <tr><br>            <td>常量</td><br>            <td>片外</td><br>            <td>有</td><br>            <td>R</td><br>            <td>所有线程+主机</td><br>            <td>主机配置</td><br>        </tr><br>        <tr><br>            <td>纹理</td><br>            <td>片外</td><br>            <td>有</td><br>            <td>R</td><br>            <td>所有线程+主机</td><br>            <td>主机配置</td><br>        </tr><br>    </tbody><br></table></p>
<h2 id="静态全局内存"><a href="#静态全局内存" class="headerlink" title="静态全局内存"></a>静态全局内存</h2><p>对于动态使用全局内存已经非常熟悉了，下面一段简短的代码展现了如何使用静态全局内存。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"cuda_runtime.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line">__device__ <span class="keyword">float</span> globalData;</span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">checkglobalData</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"Device:  recive %f from host\n"</span>, globalData);</span><br><span class="line">	globalData -= <span class="number">2.0f</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">float</span> val = <span class="number">3.14159f</span>;</span><br><span class="line">	cudaMemcpyToSymbol(globalData, &amp;val, <span class="keyword">sizeof</span>(<span class="keyword">float</span>));</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"Host:    send %f to the global variable\n"</span>, val);</span><br><span class="line">	checkglobalData &lt;&lt; &lt;<span class="number">1</span>, <span class="number">1</span> &gt;&gt; &gt;();</span><br><span class="line">	cudaDeviceSynchronize();</span><br><span class="line">	cudaMemcpyFromSymbol(&amp;val, globalData, <span class="keyword">sizeof</span>(<span class="keyword">float</span>));</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"Host:    recive %f from the global variable\n"</span>, val);</span><br><span class="line">	cudaDeviceReset();</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>运行结果如下：<br>Host: send 3.141590 to the global variable<br>Device: recive 3.141590 from host<br>Host: recive 1.141590 from the global variable</p>
<p>但是也许你会有些疑惑：cudaMemcpyToSymbol()函数的定义是：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMemcpyToSymbol</span><span class="params">(<span class="keyword">const</span> <span class="keyword">void</span> *symbol, <span class="keyword">const</span> <span class="keyword">void</span> *src, <span class="keyword">size_t</span> count)</span></span></span><br></pre></td></tr></table></figure></p>
<p>为何在传入symbol的时候不需要取地址符呢？</p>
<p>cudaMemcpyToSymbol()函数是存在于CUDA运行时API中的，在这里的globalData是一个标识符而不是确切作为它在全局内存的变量地址传入的。</p>
<p>对于核函数而言，globalData就是一个存在全局内存里的一个变量而已。</p>
<p>由于在这里的globalData对于host端而言，并不是一个变量，只是对CUDA运行时有效的一个标识符而已，所以自然<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaMemcpy(&amp;globalData,&amp;val,<span class="keyword">sizeof</span>(<span class="keyword">float</span>),cudaMemcpyHostToDevice);</span><br></pre></td></tr></table></figure></p>
<p>也是无效的，而且还会报错。</p>
<p>虽然不能显式地使用&amp;来取得地址，但是可以通过下面一个CUDA API来获取一个全局变量的地址：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaGetSymbolAddress</span><span class="params">(<span class="keyword">void</span>** devPtr,<span class="keyword">const</span> <span class="keyword">void</span>* symbol)</span></span>;</span><br></pre></td></tr></table></figure>
<p>这样获取到全局变量的物理地址之后，就可以使用cudaMemcpy函数了，所以：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaMemcpyToSymbol(globalData, &amp;val, <span class="keyword">sizeof</span>(<span class="keyword">float</span>));</span><br></pre></td></tr></table></figure>
<p>等价于<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">float</span> *dptr = <span class="literal">nullptr</span>;</span><br><span class="line">cudaGetSymbolAddress((<span class="keyword">void</span>**)&amp;dptr, globalData);</span><br><span class="line">cudaMemcpy(dptr, &amp;val, <span class="keyword">sizeof</span>(<span class="keyword">float</span>), cudaMemcpyHostToDevice);</span><br></pre></td></tr></table></figure></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/12/30/0x08/" data-id="cjf14qotv0009xstm12gqkhqm" class="article-share-link">Teilen</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-0x06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/12/28/0x06/" class="article-date">
  <time datetime="2017-12-27T16:00:00.000Z" itemprop="datePublished">2017-12-28</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/12/28/0x06/">0x06 浅谈GPU架构</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>与CUDA的线程的三个层级类似，硬件上也是三个层级，分别是：</p>
<ul>
<li>CUDA核心</li>
<li>SM流式多处理器</li>
<li>设备</li>
</ul>
<p>在feimi架构中，一个SM的关键组件：</p>
<ul>
<li>CUDA核心</li>
<li>共享内存/一级缓存</li>
<li>寄存器文件</li>
<li>加载/储存单元</li>
<li>特殊功能单元(SFU)</li>
<li>线程束调度器</li>
</ul>
<p>SM的特性有：</p>
<ul>
<li>每个SM支持数百个线程并发地执行</li>
<li>每个GPU有多个SM</li>
<li>调用核函数时，网格内的线程块会被调度在可用的SM上并发执行</li>
<li>一个线程块被指定了SM之后，其中线程只会在这个SM上执行，但是一个SM却可以容纳多个线程块。</li>
</ul>
<p>以上SM的特性可以解释，为何一个核函数里面，相同的block里的线程可以相互干涉和访问相同的共享内存块，不同的block里的就不行。而且CUDA采用单指令多线程(SIMT)架构管理和执行线程，每32个线程为一组，称之为线程束(warp)。</p>
<h2 id="线程束的执行"><a href="#线程束的执行" class="headerlink" title="线程束的执行"></a>线程束的执行</h2><p>线程束是SM的基本执行单元，当一个grid被启动的时候，线程块就会被调度到各个可用的SM中，且一个线程块只会在一个SM上执行。在线程块被分配到SM中之后，线程块会被进一步的被划分为线程束，前面说到了，一个线程束是32个连续线程组成，而且线程束中的所有线程按照单指令多线程方式(SIMT)执行。也就是说，所有的线程执行相同的指令，但操作各自的数据。</p>
<h3 id="那么现在问题就来了"><a href="#那么现在问题就来了" class="headerlink" title="那么现在问题就来了"></a>那么现在问题就来了</h3><ol>
<li><p>如果我的一个线程块里面的线程数量不是32的整数倍的话？</p>
<p> 那么这个线程块被划分的线程束的数量就是线程总数除以32之后，向上取整。也就是说如果有33个线程，依然要占两个线程束</p>
</li>
<li><p>如果我有if-else等分支语句会怎样。</p>
<p> GPU不像CPU，有着复杂的硬件用于分支预测。GPU把几乎所有芯片面积用于计算单元了。而且一个线程束里面的所有线程执行的是相同的语句。所以当遇到if-else的时候，就会使得if条件为真的线程执行if下面的子句，然后其他线程停止等待。然后让else下面的子句执行，if条件为真的线程等待。下面借用一下CUDA C编程权威指南的代码：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">mathKernel1</span><span class="params">(<span class="keyword">float</span> *c)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> tid = blockIdx.x*blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="keyword">float</span> a, b;</span><br><span class="line">    a = b = <span class="number">0.0f</span>;</span><br><span class="line">    <span class="keyword">if</span> (tid % <span class="number">2</span> == <span class="number">0</span>)</span><br><span class="line">        a = <span class="number">100.0f</span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        b = <span class="number">200.0f</span>;</span><br><span class="line">    c[tid] = a + b;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">mathKernel2</span><span class="params">(<span class="keyword">float</span> *c)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> tid = blockIdx.x*blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="keyword">float</span> a, b;</span><br><span class="line">    a = b = <span class="number">0.0f</span>;</span><br><span class="line">    <span class="keyword">if</span> ((tid / warpSize) % <span class="number">2</span> == <span class="number">0</span>)</span><br><span class="line">        a = <span class="number">100.0f</span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        b = <span class="number">200.0f</span>;</span><br><span class="line">    c[tid] = a + b;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>第一个核函数中，就会导致先执行偶数号线程，奇数号线程等待，然后再执行奇数号线程，偶数号等待。<br>而CUDA中自带的warpSize，即线程束的大小，由于线程在线程束中是线性排列，我们可以利用这个变量使得分支的粒度增大为一个线程束，以避免分支造成的设备利用率不高。利用nvprof工具可以测试你的CUDA核函数的分支分化，只需要给nvprof提供参数”–metrics branch_efficiency”即可，其中–metrics是指进行一些特殊测试，也可以简写成-m，branch_efficiency是给定测试项目，详细内容请自行参考nvprof –help</p>
<p><img src="http://t1.aixinxi.net/o_1c2fqc7va2rh1f8b1ed11p7716dma.png-w.jpg"></p>
<p>如我们所愿，第一个函数的分支效率正好是50%</p>
<h2 id="资源分配"><a href="#资源分配" class="headerlink" title="资源分配"></a>资源分配</h2><p>线程束的本地执行上下文由以下资源组成：</p>
<ul>
<li>程序计数器</li>
<li>寄存器</li>
<li>共享内存</li>
</ul>
<p>但是我们的SM的资源并非无限的，下表列出了不同计算能力的GPU所能提供的资源：</p>
<table><br>    <tbody><br>        <tr align="center"><br>            <th rowspan="2">技术条件</th><br>            <th colspan="4">计算能力</th><br>        </tr><br>        <tr align="center"><br>            <th>2.0</th><br>            <th>2.1</th><br>            <th>3.0</th><br>            <th>3.5</th><br>        </tr><br>        <tr align="center"><br>            <td>每个线程块的最大线程数</td><br>            <td colspan="4">1024</td><br>        </tr><br>        <tr align="center"><br>            <td>每个SM并发线程块最大数量</td><br>            <td colspan="2">8</td><br>            <td colspan="2">16</td><br>        </tr><br>        <tr align="center"><br>            <td>每个SM并发线程束的最大数量</td><br>            <td colspan="2">48</td><br>            <td colspan="2">64</td><br>        </tr><br>        <tr align="center"><br>            <td>每个SM并发线程的最大数量</td><br>            <td colspan="2">1536</td><br>            <td colspan="2">2048</td><br>        </tr><br>        <tr align="center"><br>            <td>每个SM中32为寄存器数量</td><br>            <td colspan="2">32KB</td><br>            <td colspan="2">64KB</td><br>        </tr><br>        <tr align="center"><br>            <td>每个线程中32位寄存器最大数量</td><br>            <td colspan="2">63</td><br>            <td colspan="2">255</td><br>        </tr><br>        <tr align="center"><br>            <td>每个SM中共享内存最大数量</td><br>            <td colspan="4">48KB</td><br>        </tr><br>    </tbody><br></table>

<p>可以看到每个SM最多并发2048个线程，而线程块最多16个，每个线程块最多1024个线程。意味着我们需要自行调节，是更大线程块，还是更小的线程块。更大的线程块的优势显而易见，可以让更多的线程使用相同的共享内存块，相互干涉。但是~</p>
<p>资源回限制SM中常驻的线程块，如果这个SM没有足够的寄存器或者共享内存供一整个块使用，那么这个内核就无法启动。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/12/28/0x06/" data-id="cjf14qotr0006xstm0teu829e" class="article-share-link">Teilen</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-0x04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/12/28/0x04/" class="article-date">
  <time datetime="2017-12-27T16:00:00.000Z" itemprop="datePublished">2017-12-28</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/12/28/0x04/">0x04 探究CUDA运行原理</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>首先，CUDA程序的典型的运行过程，在前两期都有说到，最核心的主要有3步：</p>
<ol>
<li>把数据从内存拷贝至GPU缓存</li>
<li>调用核函数</li>
<li>从GPU缓存中把运算结果拷贝至内存</li>
</ol>
<p>而对GPU缓存的操作，主要通过4个函数实现：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMalloc</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function">cudaError_t <span class="title">cudaFree</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function">cudaError_t <span class="title">cudaMemcpy</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function">cudaError_t <span class="title">cudaMemset</span><span class="params">()</span></span>;</span><br></pre></td></tr></table></figure></p>
<p>他们和标准C中的对应函数都极其相似，而且通过返回错误码来抛出错误而不是使用异常，而这个错误码，在函数成功执行的时候，应该是 cudaSuccess ，而执行失败的时候，则为 cudaErrorMemoryAllocation。还有，对于cuda函数抛出的 cudaError_t错误码，可以通过char* cudaGetErrorString() 函数来获得可读的错误消息。</p>
<hr>
<p>##CUDA层级结构简述</p>
<p><img src="http://t1.aixinxi.net/o_1c2dp3lfspr21ughstkiam1aoga.png-w.jpg"></p>
<p>如图所示，在CUDA程序中，host和device是相互独立的，host并不清楚GPU里面是什么，GPU也不知道给他发来了任务的是什么。这样的话，在下面这种情况，程序就会崩溃：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> *hostPointer;</span><br><span class="line"><span class="keyword">int</span> *devicePointer;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">省略N行</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">hostPointer=devicePointer   <span class="comment">//从device中获取计算结果</span></span><br><span class="line">                            <span class="comment">//但是应该使用cudaMemcpy(hostPointer,devicePointer,size,cudeMemcpyDevicetoHost);</span></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">省略N行</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure></p>
<p>这个的本意是从devicePointer中获取计算结果，但是执行到那一步的时候，程序就会崩溃。因为devicePointer指向的内存，对GPU缓存来说，是一个合法的地址。而这行程序是在CPU上运行的，计算机去找这个地址所指向的页时就会发现这个地址对本程序来说是非法访问，于是整个程序就 crash掉了。</p>
<hr>
<p>在CUDA这里，主要分为以下层级：</p>
<ul>
<li>device<ul>
<li>grip<ul>
<li>block<ul>
<li>thread</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="目前我们只谈论单个GPU的状况"><a href="#目前我们只谈论单个GPU的状况" class="headerlink" title="目前我们只谈论单个GPU的状况"></a>目前我们只谈论单个GPU的状况</h3><h1 id="内存层级"><a href="#内存层级" class="headerlink" title="内存层级"></a>内存层级</h1><p>由一个核函数启动的所有线程统称为一个grip，叫做网格的原因在于，block以矩阵的形式在其中排列，而thread也以矩阵的形式在block之中排列。同一个网格里面的所有线程共享一个全局内存块，而同一个块里面的全部线程共享一个共享内存块，而不同块的线程之间无法相互协作。</p>
<h3 id="线程层级"><a href="#线程层级" class="headerlink" title="线程层级"></a>线程层级</h3><p>那么这些不同的线程，是如何区分彼此的呢。主要是依靠</p>
<ul>
<li>threadIdx 线程在块中的索引</li>
<li>blockIdx 块在网格中的索引</li>
</ul>
<p>而这两个通过x,y,z三个字段来获取他的索引。那么，知道目标线程在块内的坐标，如何获取矩阵的维度呢？</p>
<ul>
<li>blockDim 块的维度</li>
<li>gridDim 网格的维度</li>
</ul>
<p>其对应的值也是通过x,y,z字段获取。<br>因此，在CUDA示例代码中的<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> i = threadIdx.x;</span><br></pre></td></tr></table></figure></p>
<p>即获取线程的id，在这里核函数调用参数为&lt;&lt;&lt;1,size&gt;&gt;&gt;，因此没有块索引<br>那么在之前的矩阵相乘代码之中，获取计算矩阵元素的x和y坐标轴的代码<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> row = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line"><span class="keyword">int</span> col = blockIdx.x * blockDim.x + threadIdx.x;</span><br></pre></td></tr></table></figure></p>
<p>本块在第几行 * 每块的线程有几行 + 本线程在当前块的第几行 = 本线程在当前所有线程的第几行</p>
<hr>
<p>最后为了照顾部分智商不如蟑螂的虫族选手，再附上一张图<br><img src="http://t1.aixinxi.net/o_1c2doaqlu1ghjq051009d531qbba.png-j.jpg"><br>如图所示的，最大的实线包围的就是grip，其中的8*8的虚线就是block，那么绿色线程就可以表示为：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> y = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line"><span class="keyword">int</span> x = blockIdx.x * blockDim.x + threadIdx.x;</span><br></pre></td></tr></table></figure></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/12/28/0x04/" data-id="cjf14qoth0002xstm80b7fzf2" class="article-share-link">Teilen</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-0x03" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/12/28/0x03/" class="article-date">
  <time datetime="2017-12-27T16:00:00.000Z" itemprop="datePublished">2017-12-28</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/12/28/0x03/">0x03 上升维度，矩阵相乘</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>在昨晚一晚的摸鱼之后<del>被雷爷吊打</del>，开始今天的工作吧。</p>
<hr>
<p>一谈起并行计算，就不得不说矩阵，这个让人又爱又恨的东西。在线性代数课上，刚刚被高等数学里无限的求积、判断是否连榨干脑浆的我们，线性代数可谓一股清流。计算就仅是简单的加减乘除，求求行列式的值真的和高等数学相比轻松太多，直到——矩阵求积<br>枯燥的加减乘除，高达O(N^3)的复杂度，造成了仅仅计算一个四阶矩阵相乘就累得手又酸又麻。</p>
<hr>
<p>正事分割线</p>
<hr>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"cuda_runtime.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"device_launch_parameters.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdlib&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> BLOCK_SIZE = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> GRID_SIZE = <span class="number">2</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> N = BLOCK_SIZE*GRID_SIZE;</span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">mulKernel</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> *da, <span class="keyword">const</span> <span class="keyword">int</span> *db, <span class="keyword">int</span> *dc)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> row = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line">	<span class="keyword">int</span> col = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i&lt;N; i++)</span><br><span class="line">	&#123;</span><br><span class="line">		sum += da[row*N + i] * db[N*i + col];</span><br><span class="line">	&#125;</span><br><span class="line">	dc[row*N + col] = sum;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="comment">//开辟内存空间</span></span><br><span class="line">	<span class="keyword">int</span> *ha, *hb, *hc;</span><br><span class="line">	<span class="keyword">int</span> *da, *db, *dc;</span><br><span class="line">	ha = <span class="keyword">new</span> <span class="keyword">int</span>[N*N];</span><br><span class="line">	hb = <span class="keyword">new</span> <span class="keyword">int</span>[N*N];</span><br><span class="line">	hc = <span class="keyword">new</span> <span class="keyword">int</span>[N*N];</span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> x = <span class="number">0</span>; x &lt; N; x++)</span><br><span class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> y = <span class="number">0</span>; y &lt; N;y++)</span><br><span class="line">		&#123;</span><br><span class="line">			ha[x*N + y] = rand() % <span class="number">10</span>;</span><br><span class="line">			hb[x*N + y] = rand() % <span class="number">10</span>;</span><br><span class="line">		&#125;</span><br><span class="line">	cudaMalloc(&amp;da, N*N*<span class="keyword">sizeof</span>(<span class="keyword">int</span>));</span><br><span class="line">	cudaMalloc(&amp;db, N*N*<span class="keyword">sizeof</span>(<span class="keyword">int</span>));</span><br><span class="line">	cudaMalloc(&amp;dc, N*N*<span class="keyword">sizeof</span>(<span class="keyword">int</span>));</span><br><span class="line"></span><br><span class="line">	<span class="comment">//拷贝内存，定义核函数调用所需执行参数</span></span><br><span class="line">	cudaMemcpy(da, ha, N*N*<span class="keyword">sizeof</span>(<span class="keyword">int</span>), cudaMemcpyHostToDevice);</span><br><span class="line">	cudaMemcpy(db, hb, N*N*<span class="keyword">sizeof</span>(<span class="keyword">int</span>), cudaMemcpyHostToDevice);</span><br><span class="line">	<span class="function">dim3 <span class="title">blockSize</span><span class="params">(BLOCK_SIZE, BLOCK_SIZE)</span></span>;</span><br><span class="line">	<span class="function">dim3 <span class="title">grid</span><span class="params">(GRID_SIZE, GRID_SIZE)</span></span>;</span><br><span class="line"></span><br><span class="line">	mulKernel &lt;&lt; &lt;grid, blockSize &gt;&gt; &gt;(da, db, dc);</span><br><span class="line"></span><br><span class="line">	<span class="comment">//从device拷贝回host中并显示出来</span></span><br><span class="line">	cudaMemcpy(hc, dc, N*N*<span class="keyword">sizeof</span>(<span class="keyword">int</span>), cudaMemcpyDeviceToHost);</span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> x = <span class="number">0</span>; x &lt; N; x++)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> y = <span class="number">0</span>; y &lt; N; y++)</span><br><span class="line">			<span class="built_in">printf</span>(<span class="string">"%d "</span>, ha[x*N + y]);</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"\n"</span>);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"\n"</span>);</span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> x = <span class="number">0</span>; x &lt; N; x++)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> y = <span class="number">0</span>; y &lt; N; y++)</span><br><span class="line">			<span class="built_in">printf</span>(<span class="string">"%d "</span>, hb[x*N + y]);</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"\n"</span>);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"\n"</span>);</span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> x = <span class="number">0</span>; x &lt; N; x++)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> y = <span class="number">0</span>; y &lt; N; y++)</span><br><span class="line">			<span class="built_in">printf</span>(<span class="string">"%d "</span>, hc[x*N + y]);</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"\n"</span>);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">//CPU验算一下</span></span><br><span class="line">	<span class="keyword">int</span> *hd = <span class="keyword">new</span> <span class="keyword">int</span>[N*N];</span><br><span class="line">	<span class="built_in">memset</span>(hd, <span class="number">0</span>, N*N*<span class="keyword">sizeof</span>(<span class="keyword">int</span>));</span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> row = <span class="number">0</span>; row &lt; N; row++)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> col = <span class="number">0</span>; col &lt; N; col++)</span><br><span class="line">		&#123;</span><br><span class="line">			<span class="keyword">for</span> (<span class="keyword">int</span> z = <span class="number">0</span>; z &lt; N; z++)</span><br><span class="line">			&#123;</span><br><span class="line">				hd[row*N + col] += ha[row*N + z] * hb[z*N + col];</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;<span class="built_in">printf</span>(<span class="string">"\n"</span>);</span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> x = <span class="number">0</span>; x &lt; N; x++)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> y = <span class="number">0</span>; y &lt; N; y++)</span><br><span class="line">			<span class="built_in">printf</span>(<span class="string">"%d "</span>, hd[x*N + y]);</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"\n"</span>);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">delete</span>[]ha;</span><br><span class="line">	<span class="keyword">delete</span>[]hb;</span><br><span class="line">	<span class="keyword">delete</span>[]hc;</span><br><span class="line">	<span class="keyword">delete</span>[]hd;</span><br><span class="line">	cudaFree(da);</span><br><span class="line">	cudaFree(db);</span><br><span class="line">	cudaFree(dc);</span><br><span class="line">	cudaDeviceReset();</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>总之还是在上次那样的套路： 开辟内存空间-&gt;把数据拷贝到GPU缓存上-&gt;执行核函数-&gt;把结果从GPU缓存拷贝回我们的内存-&gt;free开辟的内存-&gt;done<br>为了先看看计算是否有误，我这里先用的2阶矩阵，方便自己手动验算一下<del>不想重复线性代数课上的噩梦</del><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">1 4</span><br><span class="line">9 8</span><br><span class="line"></span><br><span class="line">7 0</span><br><span class="line">4 8</span><br><span class="line"></span><br><span class="line">23 32</span><br><span class="line">95 64</span><br><span class="line"></span><br><span class="line">23 32</span><br><span class="line">95 64</span><br><span class="line">请按任意键继续. . .</span><br></pre></td></tr></table></figure></p>
<p>not bad，看来计算结果没有错误，那么把计算量提升一点点</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> BLOCK_SIZE = <span class="number">16</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> GRID_SIZE = <span class="number">20</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> N = BLOCK_SIZE*GRID_SIZE;</span><br></pre></td></tr></table></figure>
<table><br>    <tbody><br>        <tr><br>            <th>矩阵阶数</th><br>            <th>CUDA计算时间(毫秒)</th><br>            <th>CPU计算时间(毫秒)</th><br>        </tr><br>        <tr><br>            <td>320</td><br>            <td>20</td><br>            <td>109</td><br>        </tr><br>        <tr><br>            <td>800</td><br>            <td>400</td><br>            <td>1846</td><br>        </tr><br>        <tr><br>            <td>1600</td><br>            <td>707</td><br>            <td>18113</td><br>        </tr><br>    </tbody><br></table>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/12/28/0x03/" data-id="cjf14qotk0003xstmq9qn0s1w" class="article-share-link">Teilen</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-0x05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/12/28/0x05/" class="article-date">
  <time datetime="2017-12-27T16:00:00.000Z" itemprop="datePublished">2017-12-28</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/12/28/0x05/">0x05 详解核函数</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="核函数定义"><a href="#核函数定义" class="headerlink" title="核函数定义"></a>核函数定义</h2><p>核函数是C语言中对device的调用的入口，其定义如下<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kernel_name &lt;&lt;&lt;grid, block&gt;&gt;&gt;(argument <span class="built_in">list</span>)</span><br></pre></td></tr></table></figure></p>
<p>grid参数指定了本函数有多少个块。而block参数则定义了每个块中有多少个线程，也就是说这个函数会被执行grid * block次</p>
<h2 id="编写核函数"><a href="#编写核函数" class="headerlink" title="编写核函数"></a>编写核函数</h2><p>在编写一个核函数的时候，在原本的函数声明前面需要加上一个限定符，其定义如下：</p>
<table><br>    <tbody><br>        <tr><br>            <th>限定符</th><br>            <th>执行端</th><br>            <th>调用端</th><br>        </tr><br>        <tr><br>            <td><br>                <strong>global</strong><br>            </td><br>            <td>由device执行</td><br>            <td>由host调用或者是计算能力为3的device调用</td><br>        </tr><br>        <tr><br>            <td><br>                <strong>device</strong><br>            </td><br>            <td>由device执行</td><br>            <td>仅由device调用</td><br>        </tr><br>        <tr><br>            <td><br>                <strong>host</strong><br>            </td><br>            <td>由host执行</td><br>            <td>仅有host调用</td><br>        </tr><br>    </tbody><br></table>

<p>有两点需要注意</p>
<ul>
<li>host限定符可以省略</li>
<li><strong>device</strong>和<strong>host</strong>可以一起使用，这样的函数就可以同时在host和device上编译<h2 id="CUDA核函数的限制"><a href="#CUDA核函数的限制" class="headerlink" title="CUDA核函数的限制"></a>CUDA核函数的限制</h2></li>
<li>只能访问设备内存</li>
<li>返回类型必须为void</li>
<li>不支持可变参数</li>
<li>不支持静态变量</li>
<li>异步调用</li>
</ul>
<p>其中最后一项最为重要，核函数调用之后，host端会立即执行下一条语句，因此需要<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaDeviceSynchronize</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br></pre></td></tr></table></figure></p>
<p>函数来等待GPU执行命令完毕。</p>
<h2 id="验证核函数"><a href="#验证核函数" class="headerlink" title="验证核函数"></a>验证核函数</h2><p>这就比较简单了，有好几种验证方式</p>
<ul>
<li>编写一个逻辑相同的普通函数，然后比较两个的运算结果，可以使用kernel&lt;&lt;&lt;1,1&gt;&gt;&gt;来使得核函数仅仅执行一次。</li>
<li>核函数之中可以调用printf()，因此可以利用printf()来观察变量<h2 id="衡量核函数的性能"><a href="#衡量核函数的性能" class="headerlink" title="衡量核函数的性能"></a>衡量核函数的性能</h2></li>
<li><p>利用C语言</p>
<p>就像之前对我们的矩阵相乘函数那样，利用clock()函数，或者是gettimeofday()函数获取较为精确的时间</p>
</li>
<li><p>nvprof工具</p>
<p>从CUDA5.0版本开始，CUDA就自带了一个性能分析工具nvprof，调用方法如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ nvprof [nvprof_args] &lt;application&gt; [application_args]</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>利用下面的命令获取帮助信息<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ nvprof --help</span><br></pre></td></tr></table></figure></p>
<p>最简单的使用方法，就是不输入任何参数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ nvprof ./test.exe</span><br></pre></td></tr></table></figure></p>
<p>以下是我运行nvprof得到的结果</p>
<p><img src="http://t1.aixinxi.net/o_1c2e4c4od19h015n3164o1dgm3roa.png-w.jpg"></p>
<p>输出的前半部分是来自程序本身，profiling result那一行以及之后的就是分析结果了。</p>
<p>可以看到主要由两方面组成，GPU占用和API调用。可以看到本例中GPU绝大多数被计算的核函数占用了，但是部分不太适合并行计算的项目中，比如两个大型的一维数组相加：</p>
<p><img src="http://t1.aixinxi.net/o_1c2e5a9m5pnb1h87kejhjk4c6a.png-w.jpg"></p>
<p>就会发现相当多的时间花在拷贝内存上面了，真正执行任务的时间只有一半左右。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/12/28/0x05/" data-id="cjf14qotl0004xstm4cqv60x0" class="article-share-link">Teilen</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-0x01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/12/26/0x01/" class="article-date">
  <time datetime="2017-12-25T16:00:00.000Z" itemprop="datePublished">2017-12-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/12/26/0x01/">0x01 出师不利</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>为此重装了下电脑，本打算赶赶时髦装VS2017+CUDA9，可这就坏事了，VS2017的编译器版本刚刚好比CUDA9的头文件里面的需求版本最大值大了1，导致编译中断。无可奈何装回了VS2015(付盗版Resharper)</p>
<p>WIN10的英文输入法，不知为何@变成了”，这个可要了老命了啊，中文输入法不小心碰到shift键就很闹心。最后折腾了一下发现，同样是english，UK的shift+2就是引号，而US的就是@，不得不佩服英国佬真的会玩。</p>
<hr>
<p>最后一个就是VS里面的intellisense会把CUDA的&lt;&lt;&lt;认成一个&lt;&lt;和一个&lt;，看着也很闹心，不过最后还是解决了，付解决办法：</p>
<hr>
<p>工具-〉选项-〉文本编辑器-〉c/c++-&gt;高级-〉禁用自动更新改为：TRUE</p>
<hr>
<p>下期开始干正事</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/12/26/0x01/" data-id="cjf14qosw0000xstmyeus885m" class="article-share-link">Teilen</a>
      
      
    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">weiter &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archiv</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">三月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">一月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">十二月 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">letzter Beitrag</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/03/18/0x11/">0x11 OPENCL编程指南前言</a>
          </li>
        
          <li>
            <a href="/2018/03/09/0x10/">数学笔记</a>
          </li>
        
          <li>
            <a href="/2018/01/01/0x09/">0x09 CUDA内存管理</a>
          </li>
        
          <li>
            <a href="/2017/12/30/0x07/">0x07 延迟隐藏和GPU占用率</a>
          </li>
        
          <li>
            <a href="/2017/12/30/0x08/">0x08 CUDA内存模型</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 smtdbd<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>