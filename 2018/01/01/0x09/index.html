<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>0x09 CUDA内存管理 | 机器学习从入门到放弃</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="CUDA的内存管理与C语言的类似，需要程序员显式地进行内存的分配、释放、移动，在此之前，已经使用过了四个CUDA内存相关的接口：1234cudaError_t cudaMalloc(T **devPtr, size_t size);cudaError_t cudaFree(void *devPtr);cudaError_t cudaMemset(void *devPtr, int value, s">
<meta property="og:type" content="article">
<meta property="og:title" content="0x09 CUDA内存管理">
<meta property="og:url" content="http://yoursite.com/2018/01/01/0x09/index.html">
<meta property="og:site_name" content="机器学习从入门到放弃">
<meta property="og:description" content="CUDA的内存管理与C语言的类似，需要程序员显式地进行内存的分配、释放、移动，在此之前，已经使用过了四个CUDA内存相关的接口：1234cudaError_t cudaMalloc(T **devPtr, size_t size);cudaError_t cudaFree(void *devPtr);cudaError_t cudaMemset(void *devPtr, int value, s">
<meta property="og:locale" content="zh-cn">
<meta property="og:updated_time" content="2018-03-08T09:54:44.128Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="0x09 CUDA内存管理">
<meta name="twitter:description" content="CUDA的内存管理与C语言的类似，需要程序员显式地进行内存的分配、释放、移动，在此之前，已经使用过了四个CUDA内存相关的接口：1234cudaError_t cudaMalloc(T **devPtr, size_t size);cudaError_t cudaFree(void *devPtr);cudaError_t cudaMemset(void *devPtr, int value, s">
  
    <link rel="alternate" href="/atom.xml" title="机器学习从入门到放弃" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">机器学习从入门到放弃</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">从CUDA到cuDNN到进入疯人院</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Suche"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-0x09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/01/01/0x09/" class="article-date">
  <time datetime="2017-12-31T16:00:00.000Z" itemprop="datePublished">2018-01-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      0x09 CUDA内存管理
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>CUDA的内存管理与C语言的类似，需要程序员显式地进行内存的分配、释放、移动，在此之前，已经使用过了四个CUDA内存相关的接口：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMalloc</span><span class="params">(T **devPtr, <span class="keyword">size_t</span> size)</span></span>;</span><br><span class="line"><span class="function">cudaError_t <span class="title">cudaFree</span><span class="params">(<span class="keyword">void</span> *devPtr)</span></span>;</span><br><span class="line"><span class="function">cudaError_t <span class="title">cudaMemset</span><span class="params">(<span class="keyword">void</span> *devPtr, <span class="keyword">int</span> value, <span class="keyword">size_t</span> count)</span></span>;</span><br><span class="line"><span class="function">cudaError_t <span class="title">cudaMemcpy</span><span class="params">(<span class="keyword">void</span> *dst, <span class="keyword">const</span> <span class="keyword">void</span> *src, <span class="keyword">size_t</span> count, <span class="keyword">enum</span> cudaMemcpyKind kind)</span></span>;</span><br></pre></td></tr></table></figure></p>
<p>以及cudaMemcpy函数的四个拷贝方向：</p>
<ul>
<li>cudaMemcpyHostToHost</li>
<li>cudaMemcpyHostToDevice</li>
<li>cudaMemcpyDeviceToHost</li>
<li>cudaMemcpyDeviceToDevice</li>
<li>cudaMemcpyDefault</li>
</ul>
<p><del>四大拷贝方向当然有五个</del><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"cuda_runtime.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdlib&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">unsigned</span> <span class="keyword">int</span> nsize = <span class="number">1</span> &lt;&lt; <span class="number">22</span>;</span><br><span class="line">	<span class="keyword">unsigned</span> <span class="keyword">int</span> nbytes = nsize*<span class="keyword">sizeof</span>(<span class="keyword">float</span>);</span><br><span class="line">	<span class="keyword">float</span> *h_a = (<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nbytes);</span><br><span class="line">	<span class="keyword">float</span> *d_a;</span><br><span class="line">	cudaMalloc(&amp;d_a, nbytes);</span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nsize; i++)</span><br><span class="line">		h_a[i] = <span class="number">3.14159f</span>;</span><br><span class="line">	cudaMemcpy(d_a, h_a, nbytes, cudaMemcpyHostToDevice);</span><br><span class="line">	cudaMemcpy(h_a, d_a, nbytes, cudaMemcpyDeviceToHost);</span><br><span class="line">	cudaFree(d_a);</span><br><span class="line">	<span class="built_in">free</span>(h_a);</span><br><span class="line">	cudaDeviceReset();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这是一个简单的数据拷贝的DEMO，利用nvprof可以查看它的运行情况：</p>
<table><br>    <tbody><br>        <tr><br>            <th>Type</th><br>            <th>Time(%)</th><br>            <th>Time</th><br>            <th>Calls</th><br>            <th>Avg</th><br>            <th>Min</th><br>            <th>Max</th><br>            <th>Name</th><br>        </tr><br>        <tr><br>            <td>GPU activities:</td><br>            <td>50.83%</td><br>            <td>3.5437ms</td><br>            <td>1</td><br>            <td>3.5437ms</td><br>            <td>3.5437ms</td><br>            <td>3.5437ms</td><br>            <td>[CUDA memcpy HtoD]</td><br>        </tr><br>        <tr><br>            <td></td><br>            <td>49.17%</td><br>            <td>3.4280ms</td><br>            <td>1</td><br>            <td>3.4280ms</td><br>            <td>3.4280ms</td><br>            <td>3.4280ms</td><br>            <td>[CUDA memcpy DtoH]</td><br>        </tr><br>    </tbody><br></table>

<p>可以简单的计算一下，4M个浮点数占16MB空间，大约传输速率为4.515Gb/s，然而GPU和GDDR5显存之间理论带宽高达上百Gb/s，差距是相当巨大的。</p>
<h2 id="固定内存"><a href="#固定内存" class="headerlink" title="固定内存"></a>固定内存</h2><p>主机的内存默认是pageable，也就是说内存被操作系统分页处理，这个操作会使得操作系统将主机上的虚拟内存分别移动到不同的物理位置。因此，GPU不可能在可分页主机内存上面安全地访问数据，当需要传输数据时，CUDA驱动程序首先分配一段锁定的分页或者固定的主机内存，然后将需要传递的主机原数据放在固定内存中，最后，再传输到设备内存。</p>
<p>而CUDA运行时允许程序员通过下面的指令手动分配固定主机内存：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMallocHost</span><span class="params">(<span class="keyword">void</span> **devPtr, <span class="keyword">size_t</span> count)</span></span>;</span><br></pre></td></tr></table></figure></p>
<p>分配的这些内存是页面锁定的，也就是不会被操作系统所移动，因此可以直接被设备访问，但是分配过多会导致主机的性能下降。</p>
<p>这些内存通过下面的函数释放：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaFreeHost</span><span class="params">(<span class="keyword">void</span> *ptr)</span></span></span><br></pre></td></tr></table></figure></p>
<p>接下来我们用固定内存替换掉可分页内存试试,仅仅需要把普通的malloc和free替换为cudaMallocHost和cudaFreeHost即可；</p>
<table><br>    <tbody><br>        <tr><br>            <th>Type</th><br>            <th>Time(%)</th><br>            <th>Time</th><br>            <th>Calls</th><br>            <th>Avg</th><br>            <th>Min</th><br>            <th>Max</th><br>            <th>Name</th><br>        </tr><br>        <tr><br>            <td>GPU activities:</td><br>            <td>50.40%</td><br>            <td>2.5224ms</td><br>            <td>1</td><br>            <td>2.5224ms</td><br>            <td>2.5224ms</td><br>            <td>2.5224ms</td><br>            <td>[CUDA memcpy HtoD]</td><br>        </tr><br>        <tr><br>            <td></td><br>            <td>49.60%</td><br>            <td>2.4822ms</td><br>            <td>1</td><br>            <td>2.4822ms</td><br>            <td>2.4822ms</td><br>            <td>2.4822ms</td><br>            <td>[CUDA memcpy DtoH]</td><br>        </tr><br>    </tbody><br></table>

<p>从3.5ms提升到2.5ms了，性能提升了40%，带宽提升到了6.343Gb/S</p>
<h2 id="零拷贝内存"><a href="#零拷贝内存" class="headerlink" title="零拷贝内存"></a>零拷贝内存</h2><p>一般来说，主机和设备之间不能相互访问对方的变量，但是有一个例外：零拷贝内存。这是一个主机和设备都可以访问的内存。在CUDA核函数之中使用零拷贝内存有着以下优势：</p>
<ul>
<li>设备内存不足的时候可以利用主机的内存</li>
<li>避免主机和设别之间数据的显式传输</li>
<li>提高PCIe传输率</li>
</ul>
<p>可以通过下面的函数开辟一片零拷贝内存：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaHostAlloc</span><span class="params">(<span class="keyword">void</span> **pHost, <span class="keyword">size_t</span> count, <span class="keyword">unsigned</span> <span class="keyword">int</span> flags)</span></span>;</span><br></pre></td></tr></table></figure>
<p>这个函数分配了count字节的主机内存，这片内存是页面锁定的，且可以被设备访问。这个函数分配的内存必须使用cudaFreeHost函数释放。flags参数可选项如下：</p>
<ul>
<li>cudaHostAllocDefalt</li>
</ul>
<p>使得此函数的行为和cudaMallocHost函数一致</p>
<ul>
<li>cudaHostAllocPortable</li>
</ul>
<p>使得分配的内存可以被所有CUDA上下文所使用，而不只是执行分配的那个</p>
<ul>
<li>cudaHostAllocWriteCombined</li>
</ul>
<p>可以免除cpu对内存的监视，减少将内存上的数据缓存到L1、L2中，在主机和设备之间传输时节省了时间，会导致主机读取这片内存效率低下。适用于主机只写入数据的场合</p>
<ul>
<li>cudaHostAllocMapped</li>
</ul>
<p>分配写联合的存储空间，主要用于主机到设备的传输或者通过映射页锁定空间CPU写而设备读的情况。但CPU读的效率不高。<br>在开辟零拷贝内存空间之后，可以通过下面的函数获取映射到此内存的设备指针：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaHostGetDevicePointer</span><span class="params">(<span class="keyword">void</span> **pDevice, <span class="keyword">void</span> *pHost, <span class="keyword">unsigned</span> <span class="keyword">int</span> flags)</span></span>;</span><br></pre></td></tr></table></figure>
<p>这个函数获取到的pDevice就是设备指针，可以像普通设备指针一样在设备上引用。但是要注意的是，PCIe总线速度远低于GDDR5内存速度，因此使用零拷贝内存会导致核函数性能降低。</p>
<p>下面是一个利用零拷贝内存执行两个一维数组相加的DEMO:<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdlib&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstring&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">initialData</span><span class="params">(<span class="keyword">float</span> *ip, <span class="keyword">int</span> size)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> i;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; size; i++)</span><br><span class="line">	&#123;</span><br><span class="line">		ip[i] = (<span class="keyword">float</span>)(rand() &amp; <span class="number">0xFF</span>) / <span class="number">10.0f</span>;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">sumArraysOnHost</span><span class="params">(<span class="keyword">float</span> *A, <span class="keyword">float</span> *B, <span class="keyword">float</span> *C, <span class="keyword">const</span> <span class="keyword">int</span> N)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> idx = <span class="number">0</span>; idx &lt; N; idx++)</span><br><span class="line">	&#123;</span><br><span class="line">		C[idx] = A[idx] + B[idx];</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">sumArrays</span><span class="params">(<span class="keyword">float</span> *A, <span class="keyword">float</span> *B, <span class="keyword">float</span> *C, <span class="keyword">const</span> <span class="keyword">int</span> N)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> i = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> (i &lt; N) C[i] = A[i] + B[i];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">sumArraysZeroCopy</span><span class="params">(<span class="keyword">float</span> *A, <span class="keyword">float</span> *B, <span class="keyword">float</span> *C, <span class="keyword">const</span> <span class="keyword">int</span> N)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> i = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> (i &lt; N) C[i] = A[i] + B[i];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="comment">// set up data size of vectors</span></span><br><span class="line">	<span class="keyword">int</span> ipower = <span class="number">10</span>;</span><br><span class="line">	<span class="keyword">if</span> (argc &gt; <span class="number">1</span>) ipower = atoi(argv[<span class="number">1</span>]);</span><br><span class="line">	<span class="keyword">int</span> nElem = <span class="number">1</span> &lt;&lt; ipower;</span><br><span class="line">	<span class="keyword">size_t</span> nBytes = nElem * <span class="keyword">sizeof</span>(<span class="keyword">float</span>);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"Vector size %d power %d  nbytes  %3.0f KB\n"</span>, nElem, ipower,(<span class="keyword">float</span>)nBytes / (<span class="number">1024.0f</span>));</span><br><span class="line"></span><br><span class="line">	<span class="comment">// part 1: using device memory</span></span><br><span class="line">	<span class="comment">// malloc host memory</span></span><br><span class="line">	<span class="keyword">float</span> *h_A, *h_B, *hostRef, *gpuRef;</span><br><span class="line">	h_A = (<span class="keyword">float</span> *)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">	h_B = (<span class="keyword">float</span> *)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">	hostRef = (<span class="keyword">float</span> *)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">	gpuRef = (<span class="keyword">float</span> *)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line"></span><br><span class="line">	<span class="comment">// initialize data at host side</span></span><br><span class="line">	initialData(h_A, nElem);</span><br><span class="line">	initialData(h_B, nElem);</span><br><span class="line">	<span class="built_in">memset</span>(hostRef, <span class="number">0</span>, nBytes);</span><br><span class="line">	<span class="built_in">memset</span>(gpuRef, <span class="number">0</span>, nBytes);</span><br><span class="line"></span><br><span class="line">	<span class="comment">// add vector at host side for result checks</span></span><br><span class="line">	sumArraysOnHost(h_A, h_B, hostRef, nElem);</span><br><span class="line"></span><br><span class="line">	<span class="comment">// malloc device global memory</span></span><br><span class="line">	<span class="keyword">float</span> *d_A, *d_B, *d_C;</span><br><span class="line">	cudaMalloc((<span class="keyword">float</span>**)&amp;d_A, nBytes);</span><br><span class="line">	cudaMalloc((<span class="keyword">float</span>**)&amp;d_B, nBytes);</span><br><span class="line">	cudaMalloc((<span class="keyword">float</span>**)&amp;d_C, nBytes);</span><br><span class="line"></span><br><span class="line">	<span class="comment">// transfer data from host to device</span></span><br><span class="line">	cudaMemcpy(d_A, h_A, nBytes, cudaMemcpyHostToDevice);</span><br><span class="line">	cudaMemcpy(d_B, h_B, nBytes, cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line">	<span class="comment">// set up execution configuration</span></span><br><span class="line">	<span class="keyword">int</span> iLen = <span class="number">512</span>;</span><br><span class="line">	<span class="function">dim3 <span class="title">block</span><span class="params">(iLen)</span></span>;</span><br><span class="line">	dim3 grid((nElem + block.x - 1) / block.x);</span><br><span class="line"></span><br><span class="line">	sumArrays &lt;&lt; &lt;grid, block &gt;&gt; &gt;(d_A, d_B, d_C, nElem);</span><br><span class="line"></span><br><span class="line">	<span class="comment">// copy kernel result back to host side</span></span><br><span class="line">	cudaMemcpy(gpuRef, d_C, nBytes, cudaMemcpyDeviceToHost);</span><br><span class="line">	</span><br><span class="line">	<span class="comment">// free device global memory</span></span><br><span class="line">	cudaFree(d_A);</span><br><span class="line">	cudaFree(d_B);</span><br><span class="line"></span><br><span class="line">	<span class="comment">// free host memory</span></span><br><span class="line">	<span class="built_in">free</span>(h_A);</span><br><span class="line">	<span class="built_in">free</span>(h_B);</span><br><span class="line"></span><br><span class="line">	<span class="comment">// part 2: using zerocopy memory for array A and B</span></span><br><span class="line">	<span class="comment">// allocate zerocpy memory</span></span><br><span class="line">	cudaHostAlloc((<span class="keyword">void</span> **)&amp;h_A, nBytes, cudaHostAllocMapped);</span><br><span class="line">	cudaHostAlloc((<span class="keyword">void</span> **)&amp;h_B, nBytes, cudaHostAllocMapped);</span><br><span class="line"></span><br><span class="line">	<span class="comment">// initialize data at host side</span></span><br><span class="line">	initialData(h_A, nElem);</span><br><span class="line">	initialData(h_B, nElem);</span><br><span class="line">	<span class="built_in">memset</span>(hostRef, <span class="number">0</span>, nBytes);</span><br><span class="line">	<span class="built_in">memset</span>(gpuRef, <span class="number">0</span>, nBytes);</span><br><span class="line"></span><br><span class="line">	<span class="comment">// pass the pointer to device</span></span><br><span class="line">	cudaHostGetDevicePointer((<span class="keyword">void</span> **)&amp;d_A, (<span class="keyword">void</span> *)h_A, <span class="number">0</span>);</span><br><span class="line">	cudaHostGetDevicePointer((<span class="keyword">void</span> **)&amp;d_B, (<span class="keyword">void</span> *)h_B, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">	<span class="comment">// add at host side for result checks</span></span><br><span class="line">	sumArraysOnHost(h_A, h_B, hostRef, nElem);</span><br><span class="line"></span><br><span class="line">	<span class="comment">// execute kernel with zero copy memory</span></span><br><span class="line">	sumArraysZeroCopy &lt;&lt; &lt;grid, block &gt;&gt; &gt;(d_A, d_B, d_C, nElem);</span><br><span class="line"></span><br><span class="line">	<span class="comment">// copy kernel result back to host side</span></span><br><span class="line">	cudaMemcpy(gpuRef, d_C, nBytes, cudaMemcpyDeviceToHost);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	<span class="comment">// free  memory</span></span><br><span class="line">	cudaFree(d_C);</span><br><span class="line">	cudaFreeHost(h_A);</span><br><span class="line">	cudaFreeHost(h_B);</span><br><span class="line">	<span class="built_in">free</span>(hostRef);</span><br><span class="line">	<span class="built_in">free</span>(gpuRef);</span><br><span class="line"></span><br><span class="line">	<span class="comment">// reset device</span></span><br><span class="line">	cudaDeviceReset();</span><br><span class="line">	<span class="keyword">return</span> EXIT_SUCCESS;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>利用nvprof工具可以看到零拷贝内存和传统方法的性能对比：</p>
<table><br>    <tbody><br>        <tr><br>            <th>Type</th><br>            <th>Time(%)</th><br>            <th>Time</th><br>            <th>Calls</th><br>            <th>Avg</th><br>            <th>Min</th><br>            <th>Max</th><br>            <th>Name</th><br>        </tr><br>        <tr><br>            <td>GPU activities:</td><br>            <td>21.67%</td><br>            <td>29.010ms</td><br>            <td>1</td><br>            <td>29.010ms</td><br>            <td>29.010ms</td><br>            <td>29.010ms</td><br>            <td>sumArraysZeroCopy(float<br>                <em>, float</em>, float<br>                <em>, int)</em><br>            </td><br>        </tr><br>        <tr><br>            <td></td><br>            <td>18.14%</td><br>            <td>17.601ms</td><br>            <td>1</td><br>            <td>17.601ms</td><br>            <td>17.601ms</td><br>            <td>17.601ms</td><br>            <td>sumArrays(float, float<br>                <em>, float</em>, int)</td><br>        </tr><br>    </tbody><br></table>

<p>可以看到使用零拷贝内存的函数，和传统方法相比效率稍微差了一些。因此，如果想要共享主机和设备之间的少量数据，零拷贝内存是一个不错的方法。因为它简化了代码量且有着不算太差的性能。但是太大的数据集而言，可能并不是一个很好的选择。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/01/01/0x09/" data-id="cjf14qotw000axstm6rp0gxf6" class="article-share-link">Teilen</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2018/03/09/0x10/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Neuer</strong>
      <div class="article-nav-title">
        
          数学笔记
        
      </div>
    </a>
  
  
    <a href="/2017/12/30/0x07/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Älter</strong>
      <div class="article-nav-title">0x07 延迟隐藏和GPU占用率</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archiv</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">三月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">一月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">十二月 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">letzter Beitrag</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/03/18/0x11/">0x11 OPENCL编程指南前言</a>
          </li>
        
          <li>
            <a href="/2018/03/09/0x10/">数学笔记</a>
          </li>
        
          <li>
            <a href="/2018/01/01/0x09/">0x09 CUDA内存管理</a>
          </li>
        
          <li>
            <a href="/2017/12/30/0x07/">0x07 延迟隐藏和GPU占用率</a>
          </li>
        
          <li>
            <a href="/2017/12/30/0x08/">0x08 CUDA内存模型</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 smtdbd<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>